{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Matrix decompositions and how we compute them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Matrices, vectors, matrix/vector norms, scalar products & unitary matrices  \n",
    "**Week 2:** TAs-week (Strassen, FFT, a bit of SVD)  \n",
    "**Week 3:** Matrix ranks, singular value decomposition, linear systems, eigenvalues  \n",
    "**Week 4:** Matrix decompositions: QR, LU, SVD + test + structured matrices start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Eigenvectors and eigenvalues\n",
    "- Characteristic polynomial and why it is a bad idea\n",
    "- Power method: $x := Ax$\n",
    "- Gershgorin theorem\n",
    "- Schur theorem: $A = U T U^*$ \n",
    "- Normal matrices: $A^* A = A A^*$\n",
    "- A brief whiteboard illustration of how to compute the Schur form: QR-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "Today we will talk about **matrix factorizations**\n",
    "\n",
    "- LU decomposition and Gaussian elimination\n",
    "- QR decomposition and Gram-Schmidt algorithm\n",
    "- Schur decomposition and QR-algorithm\n",
    "\n",
    "We already introduced it some time ago, but now we are going to do it in more details.  \n",
    "\n",
    "These are the basic **matrix factorizations** in numerical linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General concept of matrix factorization\n",
    "\n",
    "In numerical linear algebra we need to solve different tasks, for example:\n",
    "\n",
    "- Solve linear systems $Ax = f$\n",
    "- Compute eigenvalues / eigenvectors\n",
    "- Compute singular values / singular vectors\n",
    "- Compute inverses, even sometimes determinants \n",
    "- Compute **matrix functions** like $\\exp(A), \\cos(A)$.\n",
    "\n",
    "In order to do this, we replace the matrix as a sum and/or product of matrices with **simpler structure**,  \n",
    "such that we can solve abovementioned tasks faster / more stable form.\n",
    "\n",
    "What is a **simpler structure**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a simpler structure\n",
    "We already encountered several classes of matrices **with structure**. \n",
    "\n",
    "In dense matrix business, the most important classes are\n",
    "\n",
    "**unitary matrices** ($U^* U = I$, why?), **upper/lower triangular matrices **, **diagonal matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other classes of matrices\n",
    "\n",
    "For **sparse matrices** the sparse constraints are often included in the factorizations.  \n",
    "\n",
    "For **Toeplitz matrices** an important class of matrices is the matrices with **low displacement rank**, \n",
    "\n",
    "which is based on the low-rank matrices. \n",
    "\n",
    "The class of **low-rank matrices** and **block low-rank matrices** is everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "The plan for todays lecture is to discuss the decompositions one-by-one and point out:\n",
    "- How to compute a particular decomposition\n",
    "- When the decomposition exists \n",
    "- What is done in \"real life\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decompositions we want to discuss today\n",
    "- LU factorization\n",
    "- Positive definite matrices and Cholesky factorization\n",
    "- QR-decomposition and Gram-Schmidt algorithm\n",
    "- 1 slide about the SVD (more tomorrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the LU factorization\n",
    "\n",
    "LU factorization is the representation of a given matrix $A$ as a product  \n",
    "of a **lower triangular matrix** $L$ and an **upper triangular** matrix $U$:\n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "This factorization is **non-unique**, so it is typical to require that the matrix $L$ has ones on the diagonal.\n",
    "\n",
    "Does it always exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Main goal** of the LU decomposition is to solve linear systems, because\n",
    "\n",
    "$$\n",
    "    A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, \n",
    "$$\n",
    "\n",
    "and this reduces to the solution of two linear systems \n",
    "$$\n",
    "     L y = f, \n",
    "$$\n",
    "and \n",
    "$$\n",
    "   U x = y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solving linear systems with **triangular matrices** is **easy**: you just solve the equation with one unknown, put into the second equation, and so on.  \n",
    "\n",
    "The computational cost of such algorithm is $\\mathcal{O}(n^2)$ arithmetic operations, once $LU$ factorization is known. \n",
    "\n",
    "Note that the **inverse of triangular matrix** is a triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sympy\n",
    "from sympy.matrices import Matrix\n",
    "import IPython\n",
    "from sympy.interactive.printing import init_printing\n",
    "init_printing(use_latex=True)\n",
    "n = 5\n",
    "w = Matrix(n, n, lambda i, j: 1/(i + j +  sympy.Integer(1)/2))\n",
    "L, U, tmp = w.LUdecomposition()\n",
    "#Generate the final expression\n",
    "#fn = u'%s \\\\times %s = %s' % (sympy.latex(L), sympy.latex(U), sympy.latex(w))\n",
    "#L * U - w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The classical LU-decomposition algorithm\n",
    "The simplest way to implement LU-decomposition is the following 3-cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎡2  2/3    2/5    2/7       2/9   ⎤\n",
      "⎢                                 ⎥\n",
      "⎢           16               32   ⎥\n",
      "⎢0  8/45   ───    8/63      ───   ⎥\n",
      "⎢          105              297   ⎥\n",
      "⎢                                 ⎥\n",
      "⎢          128    128       256   ⎥\n",
      "⎢0   0    ─────   ────     ─────  ⎥\n",
      "⎢         11025   8085     15015  ⎥\n",
      "⎢                                 ⎥\n",
      "⎢                 512       2048  ⎥\n",
      "⎢0   0      0    ──────   ─────── ⎥\n",
      "⎢                693693   1486485 ⎥\n",
      "⎢                                 ⎥\n",
      "⎢                          32768  ⎥\n",
      "⎢0   0      0      0     ─────────⎥\n",
      "⎣                        703956825⎦\n"
     ]
    }
   ],
   "source": [
    "from sympy.interactive.printing import init_printing\n",
    "init_printing(use_latex=True)\n",
    "L = Matrix(n, n, lambda i, j: 0)\n",
    "U = Matrix(n, n, lambda i, j: 0)\n",
    "a = w.copy() #Our matrix\n",
    "for k in xrange(n): #Eliminate one row\n",
    "    L[k, k] = 1\n",
    "    for i in xrange(k+1, n):\n",
    "        L[i, k] = a[i, k] / a[k, k]\n",
    "        for j in xrange(k+1, n):\n",
    "            a[i, j] = a[i, j] - L[i, k] * a[k, j]\n",
    "    for j in xrange(k, n):\n",
    "        U[k, j] = a[k, j]\n",
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existence of the LU-decomposition\n",
    "The LU-decomposition algorithm does not fail  \n",
    "\n",
    "if **we do not divide by zero** at every step.\n",
    "\n",
    "When it is so, for which class of matrices?\n",
    "\n",
    "It is true for **strictly regular matrices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strictly regular matrices and LU decomposition \n",
    "A matrix $A$ is called strictly regular, if all of its **principal minors**  \n",
    "(i.e, submatrices in the first $k$ rows and $k$ columns) are non-singular. \n",
    "\n",
    "In this case, there always exists a LU-decomposition.  The reverse is also true (check!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Proof**. If there is a LU-decomposition, then the matrix is strictly regular \n",
    "\n",
    "This follows from the fact that to get a minor you multiply a corresponding submatrix of $L$ by a corresponding submatrix of $U$, and they are non-singular (since non-singularity of triangular matrices is equivalent to the fact that their diagonal elements are not equal to zero).\n",
    "\n",
    "The other way can be proven by **induction**. Suppose that we know that for all $(n-1) \\times (n-1)$ matrices will non-singular minors LU-decomposition exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, consider the block form\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "          a & c^{\\top} \\\\\n",
    "          b & D\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "where $D$ is $(n-1) \\times (n-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we do \"block elimination\": \n",
    "\n",
    "$$\n",
    "     \\begin{bmatrix}\n",
    "     1 & 0 \\\\\n",
    "     -z & I\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "     a & c^{\\top} \\\\\n",
    "     b & D \n",
    "     \\end{bmatrix}=\n",
    "     \\begin{bmatrix}\n",
    "     a & c^{\\top} \\\\\n",
    "     0 & A_1\n",
    "     \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $z = \\frac{b}{a}, \\quad A_1 = D - \\frac{1}{a} b c^{\\top}$.  \n",
    "We can show that $A_1$ is also strictly regular, thus it has (by induction) the LU-decomposition:  \n",
    "$$\n",
    "   A_1 = L_1 U_1, \n",
    "$$\n",
    "And that gives the $LU$ decomposition of the original matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When LU fails\n",
    "What happens, if the matrix is not strictly regular (or the **pivots** in the Gaussian elimination are really small?). \n",
    "\n",
    "There is classical $2 \\times 2$ example of a matrix with a bad LU decomposition.  \n",
    "\n",
    "The matrix we look at is  \n",
    "$$\n",
    "    A = \\begin{pmatrix}\n",
    "    \\varepsilon & 1 \\\\\n",
    "    1 & 1 \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If $\\varepsilon$ is sufficiently small, we **might** fail.\n",
    "\n",
    "Let us do some demo here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L * U - A: [[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eps = 1e-8\n",
    "a = [[eps, 1],[1,  1]]\n",
    "a = np.array(a)\n",
    "a0 = a.copy()\n",
    "n = a.shape[0]\n",
    "L = np.zeros((n, n))\n",
    "U = np.zeros((n, n))\n",
    "for k in xrange(n): #Eliminate one row   \n",
    "    L[k, k] = 1\n",
    "    for i in xrange(k+1, n):\n",
    "        L[i, k] = a[i, k] / a[k, k]\n",
    "        for j in xrange(k+1, n):\n",
    "            a[i, j] = a[i, j] - L[i, k] * a[k, j]\n",
    "    for j in xrange(k, n):\n",
    "        U[k, j] = a[k, j]\n",
    "\n",
    "print 'L * U - A:', np.dot(L, U) - a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The concept of pivoting\n",
    "We can do pivoting, i.e. permute rows and columns to maximize $A_{kk}$ that we divide over.  \n",
    "The simplest but effective strategy is the **row pivoting**: at each step, select the index that is maximal in modulus, and put it onto the diagonal. \n",
    "\n",
    "It gives us the decomposition \n",
    "\n",
    "$$A = P L U,$$\n",
    "where $P$ is a **permutation matrix**.\n",
    "\n",
    "\n",
    "What makes **row pivoting** good?  \n",
    "\n",
    "It is made good by the fact that  \n",
    "\n",
    "$$\n",
    "   | L_{ij}|<1,\n",
    "$$\n",
    "but the elements of $U$ can grow, up to $2^n$! (in practice, this is very rarely encountered).\n",
    "\n",
    "Can you come up with a matrix where the elements of $U$ grow as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positive definite matrices\n",
    "\n",
    "Strictly regular matrices have LU-decomposition. \n",
    "\n",
    "An important **subclass** of strictly regular matrices is the class of **Hermitian positive definite matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positive definite matrices(2)\n",
    "**Definition:** A matrix is called <font color='red'> positive definite </font> if for any $x: \\Vert x \\Vert \\ne 0$ we have\n",
    "\n",
    "$$(Ax, x) > 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symmetric positive definite matrices\n",
    "\n",
    "**Statement:** A Hermitian positive definite matrix $A$ is strictly regular and has **Cholesky factorization** of the form \n",
    "\n",
    "$$A = RR^*,$$\n",
    "\n",
    "where $R$ is upper triangular.\n",
    "\n",
    "Let us try to prove this fact (on the whiteboard).\n",
    "\n",
    "The Cholesky factorization is **always stable**.\n",
    "\n",
    "It is sometimes referred to as \"square root\" of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary on the LU for dense matrices\n",
    "- Principal submatrices non-singular -- there exists LU\n",
    "- Pivoting is needed to avoid error growth\n",
    "- Complexity is $\\mathcal{O}(n^3)$ for the factorization step (can be speeded by using Strassen ideas) and $\\mathcal{O}(n^2)$ for a solve\n",
    "- For SPD matrices Cholesky factorization is the method of choice (stability, twice less memory to store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-decomposition\n",
    "The next decomposition: **QR** decomposition. Again from the name it is clear that a matrix is represented as a product  \n",
    "$$\n",
    "    A = Q R, \n",
    "$$\n",
    "where $Q$ is an **orthogonal (unitary)** matrix and $R$ is **upper triangular**.  \n",
    "\n",
    "The matrix sizes: $Q$ is $n \\times m$, $R$ is $m \\times m$.\n",
    "\n",
    "QR-factorization is defined for **any rectangular matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-decomposition: applications\n",
    "\n",
    "This algorithm plays a crucial role in many problems:\n",
    "- Computing orthogonal bases in a linear space\n",
    "- Used in the preprocessing step for the SVD\n",
    "- QR-algorithm for the computation of eigenvectors and eigenvalues (one of the 10 most important algorithms of the 20th century) is    based on the QR-decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem \n",
    "Every rectangular $n \\times m$ matrix, $n \\geq m$ matrix has a QR-decomposition. \n",
    "There are several ways to prove it and compute it.\n",
    "\n",
    "- (theoretical) Using the Gram matrices and Cholesky factorization\n",
    "- (geometrical) Using the Gram-Schmidt orthogonalization\n",
    "- (practical) Using Householder/Givens transformations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical way\n",
    "If we have the representation of the form\n",
    "$$A = QR,$$\n",
    "then $A^* A = R^* R$, the matrix $A^* A$ is called **Gram matrix**, and its elements are scalar products of the columns of $A$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Math way: full column rank\n",
    "\n",
    "Assume that $A$ has **full column rank**. Then, \n",
    "\n",
    "It is simple to show that $A^* A$ is positive definite:\n",
    "\n",
    "$$\n",
    "   (A^* A y, y) = (Ay, Ay)^2 = \\Vert Ay \\Vert  \\geq 0.\n",
    "$$\n",
    "\n",
    "Therefore, $A^* A = R^* R$ always exists.\n",
    "\n",
    "Then the matrix $A R^{-1}$ is unitary:  \n",
    "\n",
    "$$\n",
    "   (A R^{-1})^* (AR^{-1})= R^{-*} A^* A R^{-1} = R^{-*} R^* R R^{-1} = I.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rank-deficient case\n",
    "When an $n \\times m$ matrix does not have full column rank, it is said to be **rank-deficient**.  \n",
    "\n",
    "The QR-decomposition, however, also exists.\n",
    "\n",
    "For any **rank-deficient matrix** there is a sequence of full-column rank matrices $A_k$ such that $A_k \\rightarrow A$.\n",
    "\n",
    "Each $A_k$ can be decomposed as $A_k = Q_k R_k$. \n",
    "\n",
    "The set of all unitary matrices is compact, thus there exists a converging subsequence $Q_{n_k} \\rightarrow Q$,\n",
    "\n",
    "and $Q^* A_k \\rightarrow Q^* A = R$, which is triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-decomposition and Gram matrices\n",
    "So, the simplest way to compute QR-decomposition is then\n",
    "\n",
    "$$A^* A = R^* R,$$\n",
    "\n",
    "(Cholesky factorization)\n",
    "\n",
    "$$Q = A R^{-1}.$$\n",
    "\n",
    "It is a **bad idea** for numerical stability. Let us do some demo (for a submatrix of the Hilbert matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 20\n",
    "r = 5\n",
    "#a = np.random.randn(n, r)\n",
    "a = [[1.0/(i+j+0.5) for i in xrange(r)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "q, Rmat = np.linalg.qr(a)\n",
    "e = np.eye(r)\n",
    "print 'Built-in QR orth', np.linalg.norm(np.dot(q.T, q) - e)\n",
    "gram_matrix = a.T.dot(a)\n",
    "Rmat1 = np.linalg.cholesky(gram_matrix)\n",
    "q1 = np.dot(a, np.linalg.inv(Rmat1.T))\n",
    "print 'Via Gram matrix:', np.linalg.norm(np.dot(q1.T, q1) - e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second way: Gram-Schmidt orthogonalization\n",
    "QR-decomposition is a mathematical way of writing down the Gram-Schmidt orthogonalization process.  \n",
    "Given a sequence of vectors $a_1, \\ldots, a_m$ we want to find orthogonal basis $q_1, \\ldots, q_m$ such that every $a_i$ is a linear combination of such vectors.  \n",
    "\n",
    "**Gram-Schmidt:**\n",
    "1. $q_1 := a_1/\\Vert a_1 \\Vert$\n",
    "2. $q_2 := a_2 - (a_2, q_1) q_1, \\quad q_2 := q_2/\\Vert q_2 \\Vert$\n",
    "3. $q_3 := a_3 - (a_3, q_1) q_1 - (a_3, q_2) q_2, \\quad q_2 := q_3/\\Vert q_3 \\Vert$\n",
    "4. And go on  \n",
    "\n",
    "Note that the transformation from $Q$ to $A$ has triangular structure, since from the $k$-th vector we subtract only the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gram-Schmidt is unstable\n",
    "Gram-Schmidt can be very unstable (i.e., the produced vectors will be not orthogonal, especially if $q_k$ has small norm).  \n",
    "This is called **loss of orthogonality**.  \n",
    "\n",
    "There is a remedy, called **modified Gram-Schmidt** method. \n",
    "Instead of doing \n",
    "\n",
    "$$q_k := a_k - (a_k, q_1) q_1 - \\ldots - (a_k, q_{k-1}) q_{k-1}$$\n",
    "\n",
    "we do it step-by-step. First we set $q_k := a_k$ and orthogonalize sequentially:\n",
    "\n",
    "$$\n",
    "   q_k := q_k - (q_k, q_1)q_1, \\quad q_k := q_{k} - (q_k,q_2)q_2, \\ldots\n",
    "$$\n",
    "\n",
    "In exact arithmetic, it is the same. In floating point it is absolutely different!\n",
    "\n",
    "Note that the complexity is $\\mathcal{O}(nm^2)$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-decomposition: the (almost) practical way\n",
    "\n",
    "If $A = QR$, then  \n",
    "$$\n",
    "R = Q^* A,\n",
    "$$\n",
    "and we need to find a certain orthogonal matrix $Q$ that brings a matrix into orthogonal form.  \n",
    "For simplicity, we will look for an $n \\times n$ matrix such that\n",
    "$$\n",
    "   Q^* A = \\begin{bmatrix}\n",
    "   * & * & *  \\\\\n",
    "   0 & * & * \\\\\n",
    "   0 & 0 & * \\\\\n",
    "   &0_{(n-m) \\times m}\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will do it column-by-column.  \n",
    "First, we find a Householder matrix $H_1 = (I - 2 uu^{\\top})$ such that (we illustrate on a $4 \\times 3$ matrix)\n",
    "\n",
    "$$\n",
    "   H_1 A = \\begin{bmatrix}\n",
    "    * & * & * \\\\\n",
    "    0 & * & * \\\\\n",
    "    0 & * & * \\\\\n",
    "    0 & * & *\n",
    "   \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, \n",
    "$$\n",
    "   H_2 H_1 A = \\begin{bmatrix}\n",
    "    * & * & * \\\\\n",
    "    0 & * & * \\\\\n",
    "    0 & 0 & * \\\\\n",
    "    0 & 0 & *\n",
    "   \\end{bmatrix},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "  H_2 = \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & H'_2, \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "and $H'_2$ is a $3 \\times 3$ Householder matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, \n",
    "$$\n",
    "   H_3 H_2 H_1 A = \\begin{bmatrix}\n",
    "    * & * & * \\\\\n",
    "    0 & * & * \\\\\n",
    "    0 & 0 & * \\\\\n",
    "    0 & 0 & 0\n",
    "   \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "You can try to implement by yourself, it is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-decomposition: real life\n",
    "\n",
    "In reality, since this is a dense matrix factorization, you should implement the algorithm in terms of blocks.  \n",
    "\n",
    "Instead of using Householder transformation, we use **block Householder** transformation of the form\n",
    "\n",
    "$$H = (I - 2UU^*), $$\n",
    "\n",
    "where $U^* U = I$.\n",
    "\n",
    "This allows us to use BLAS-3 operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rank-revealing QR-decomposition\n",
    "\n",
    "The QR-decomposition can be also used to compute the (numerical) rank of the matrix.  \n",
    "\n",
    "It is done via so-called **rank-revealing** factorization.  \n",
    "\n",
    "It is based on the representation\n",
    "\n",
    "$$P A = Q R, $$\n",
    "\n",
    "where $P$ is the permutation matrix (it permutes columns), and  $R$ has the block form\n",
    "\n",
    "\n",
    "$$R = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22}\\end{bmatrix},$$\n",
    "\n",
    "and the norm of $R_{22}$ is **small**, so you can find the numerical rank by looking at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "LU and QR decompositions can be computed using **direct methods** in finite amount of operations.\n",
    "\n",
    "What about Schur form and SVD?  \n",
    "\n",
    "They can not be computed by **direct methods** they can only be computed by **iterative methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Schur form\n",
    "\n",
    "Every matrix can be written as a product\n",
    "\n",
    "$$A = U T U^*,$$\n",
    "\n",
    "with triangular $T$ and unitary $U$\n",
    "\n",
    "and this decomposition gives eigenvalues of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR-algorithm\n",
    "The QR algorithm (Kublanovskaya and Francis independently proposed it in 1961).   \n",
    "\n",
    "\n",
    "\n",
    "<font color='red'> Do not **mix** QR algorithm and QR decomposition. </font>\n",
    "\n",
    "QR-decomposition is the representation of a matrix, whereas QR algorithm uses QR decomposition computes the eigenvalues!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Way to QR-algorithm\n",
    "\n",
    "Consider the equation\n",
    "\n",
    "$$A = Q T Q^*,$$\n",
    "\n",
    "and rewrite it in the form\n",
    "\n",
    "$$\n",
    "   Q T = A Q.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation of the QR-algorithm as fixed-point\n",
    "Then we can write down the iterative process \n",
    "\n",
    "$$\n",
    "    Q_{k+1} R_{k+1} = A Q_k, \\quad Q_{k+1}^* A = R_{k+1} Q^*_k\n",
    "$$\n",
    "\n",
    "Introduce\n",
    "\n",
    "$$A_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \\widehat{Q}_k R_{k+1}$$.\n",
    "\n",
    "and the new approximation reads\n",
    "\n",
    "$$A_{k+1} = Q^*_{k+1} A Q_{k+1} = \\widehat{Q}^*_k A_k \\widehat{Q}_k = R_{k+1} \\widehat{Q}_k.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final formulas\n",
    "The final formulas are then written in the form\n",
    "$$\n",
    "   A_0 = A, \\quad A_k = Q_k R_k, \\quad A_{k+1} = R_k Q_k.\n",
    "$$\n",
    "\n",
    "The matrices $A_{k}$ are unitary similar, and $A_k \\rightarrow T$.  \n",
    "\n",
    "The complexity of each step is $\\mathcal{O}(n^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 4\n",
    "a = [[1.0/(i + j + 0.5) for i in xrange(n)] for j in xrange(n)]\n",
    "niters = 1\n",
    "for k in xrange(niters):\n",
    "    q, rmat = np.linalg.qr(a)\n",
    "    a = rmat.dot(q)\n",
    "    print 'current a:'\n",
    "    print a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence and complexity of the QR-algorithm\n",
    "The convergence of the QR-algorithm is from **largest eigenvalues** to the smallest.\n",
    "\n",
    "At least 2-3 iterations is needed for an eigenvalue. \n",
    "\n",
    "Each step is one QR-factorization and one matrix-by-matrix product. $\\mathcal{O}(n^3)$ complexity.\n",
    "\n",
    "It means, $\\mathcal{O}(n^4)$ complexity? \n",
    "\n",
    "Fortunately, not. \n",
    "\n",
    "We can also speedup the QR-algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few words about the SVD\n",
    "\n",
    "And the last, but not the least, is the **singular value decomposition** of matrices.\n",
    "\n",
    "$$A = U \\Sigma V^*.$$\n",
    "\n",
    "We can compute via eigendecomposition of \n",
    "\n",
    "$$A^* A = U^* \\Sigma^2 U,$$\n",
    "\n",
    "but it is a **bad idea** (c.f. Gram matrices)\n",
    "\n",
    "We will discuss methods for computing SVD later (there will a special SVD lecture).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of todays lecture\n",
    "\n",
    "- LU decomposition and Gaussian elimination\n",
    "- QR decomposition and Gram-Schmidt algorithm, reduction to a simpler form by Householder transformations\n",
    "- Schur decomposition and QR-algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Test\n",
    "- SVD: how to compute it, what are the applications, computing best rank-$r$ approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
