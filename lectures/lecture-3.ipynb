{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Matrix multiplication, scalar product, unitary matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Intro week, floating point, vector norms, matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Concept of floating point\n",
    "- Basic vector norms(p-norm, Manhattan distance, 2-norm, Chebyshev norm)\n",
    "- A short demo on $L_1$-norm minimization\n",
    "- Concept of forward/backward error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "Today we will talk about:\n",
    "- Matrices\n",
    "- Matrix multiplication\n",
    "- Matrix norms, operator norms\n",
    "- unitary matrices, unitary invariant norms\n",
    "- Concept of block algorithms for NLA: why and how.\n",
    "- Complexity of matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-by-matrix product\n",
    "\n",
    "Consider composition of two linear operators:\n",
    "\n",
    "1. $y = Bx$\n",
    "2. $z = Ay$\n",
    "\n",
    "Then, $z = Ay =  A B x = C x$, where $C$ is the **matrix-by-matrix product**.\n",
    "\n",
    "A product of an $n \\times k$ matrix $A$ and a $k \\times m$ matrix $B$ is a $n \\times m$ matrix $C$ with the elements  \n",
    "$$\n",
    "   c_{ij} = \\sum_{s=1}^k a_{is} b_{sj}, \\quad i = 1, \\ldots, n, \\quad j = 1, \\ldots, m \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexity of MM\n",
    "Complexity of a naive algorithm for MM is $\\mathcal{O}(n^3)$.   \n",
    "\n",
    "Matrix-by-matrix product is the **core** for almost all efficient algorithms in linear algebra.  \n",
    "\n",
    "Basically, all the NLA algorithms are reduced to a sequence of matrix-by-matrix products, \n",
    "\n",
    "so efficient implementation of MM reduces the complexity of numerical algorithms by the same factor.  \n",
    "\n",
    "However, implementing MM is not easy at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficient implementation for MM\n",
    "Is it easy to multiply a matrix by a matrix?  \n",
    "\n",
    "The answer is: **no**, if you want it as fast as possible,  \n",
    "\n",
    "using the computers that are at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo\n",
    "Let us do a short demo and compare a `np.dot()` procedure which in my case uses MKL with a hand-written matrix-by-matrix routine in Python and also its Cython version (and also gives a very short introduction to Cython)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matmul(a, b):\n",
    "    n = a.shape[0]\n",
    "    k = a.shape[1]\n",
    "    m = b.shape[1]\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for s in range(k):\n",
    "                c[i, j] += a[i, s] * b[s, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext cythonmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "def cython_matmul(double [:, :] a, double[:, :] b):\n",
    "    cdef int n = a.shape[0]\n",
    "    cdef int k = a.shape[1]\n",
    "    cdef int m = b.shape[1]\n",
    "    cdef int i\n",
    "    cdef int j \n",
    "    cdef int s\n",
    "    c = np.zeros((n, m))\n",
    "    cdef double[:, :] cview = c\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for s in range(k):\n",
    "                c[i, j] += a[i, s] * b[s, j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we just compare computational times.\n",
    "\n",
    "Guess the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = np.random.randn(n, n)\n",
    "b = np.random.randn(n, n)\n",
    "%timeit c = matmul(a, b)\n",
    "%timeit cf = cython_matmul(a, b)\n",
    "%timeit c = np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why it is so?   \n",
    "There are two important issues:\n",
    "\n",
    "- Computers are more and more parallel (multicore, graphics processing units)\n",
    "- The memory pyramid: there is a whole hierarchy of levels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory architecture\n",
    "<img width=80% src=\"Computer_Memory_Hierarchy.svg\">\n",
    "Fast memory is small, bigger memory is slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Data fits into the  fast memory:  \n",
    "  load all data, compute\n",
    "- Data does not fit into the fast memory:  \n",
    "  load data by chunks, compute, load again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to reduce the number of read/write operations!  \n",
    "\n",
    "This is typically achieved in efficient implementations of the BLAS libraries, one of which (Intel MKL) we now use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BLAS\n",
    "Basic linear algebra operations (**BLAS**) have three levels:\n",
    "1. BLAS-1, operations like $c = a + b$\n",
    "2. BLAS-2, operations like matrix-by-vector product\n",
    "3. BLAS-3, matrix-by-matrix product\n",
    "\n",
    "What is the principal differences between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main difference is the number of operations vs. the number of input data!\n",
    "\n",
    "1. BLAS-1: $\\mathcal{O}(n)$ data, $\\mathcal{O}(n)$ operations\n",
    "2. BLAS-2: $\\mathcal{O}(n^2)$ data, $\\mathcal{O}(n^2)$ operations\n",
    "3. BLAS-3: $\\mathcal{O}(n^2)$ data, $\\mathcal{O}(n^3)$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remark**: a quest for $\\mathcal{O}(n^2)$ matrix-by-matrix multiplication algorithm is not yet done.\n",
    "\n",
    "Strassen gives $\\mathcal{O}(n^{2.78...})$   \n",
    "\n",
    "World record $\\mathcal{O}(n^{2.37})$ [Reference](http://arxiv.org/pdf/1401.7714v1.pdf)  \n",
    "\n",
    "The constant is unfortunately too big to make it practical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory hierarchy\n",
    "How we can use memory hierarchy? \n",
    "<img src=\"Computer_Memory_Hierarchy.svg\" width = 70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Break the matrix into blocks! ($2 \\times 2$ is an **illustration**)  \n",
    "\n",
    "$\n",
    "   A = \\begin{bmatrix}\n",
    "         A_{11} & A_{12} \\\\\n",
    "         A_{21} & A_{22}\n",
    "        \\end{bmatrix}$, $B = \\begin{bmatrix}\n",
    "         B_{11} & B_{12} \\\\\n",
    "         B_{21} & B_{22}\n",
    "        \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then,  \n",
    "\n",
    "$AB$ = $\\begin{bmatrix}A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\\\\n",
    "            A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\end{bmatrix}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If $A_{11}, B_{11}$ and their product fit into the cache memory (which is 1024 Kb for the [Haswell Intel Chip](http://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#.22Haswell-H.22_.28MCP.2C_quad-core.2C_22_nm.29)), then we load them only once into the memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key point\n",
    "The number of read/writes is reduced by a factor $\\sqrt{M}$, where $M$ is the cache size.  \n",
    "\n",
    "- Have to do linear algebra in terms of blocks! \n",
    "- So, you can not even do Gaussian elimination as usual (or just suffer 10x performance loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelization\n",
    "The blocking has also deep connection with parallel computations.  \n",
    "Consider adding two vectors:\n",
    "$$ c = a + b$$\n",
    "and we have two processors.  \n",
    "\n",
    "How fast can we go?  \n",
    "\n",
    "Of course, not faster then twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 60.65 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100000 loops, best of 3: 1.25 µs per loop\n",
      "The slowest run took 32.57 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1000000 loops, best of 3: 1.29 µs per loop\n"
     ]
    }
   ],
   "source": [
    "## This demo requires Anaconda distribution to be installed\n",
    "import mkl\n",
    "import numpy as np\n",
    "n = 1000\n",
    "a = np.random.randn(n)\n",
    "mkl.set_num_threads(1)\n",
    "%timeit a + a\n",
    "mkl.set_num_threads(2)\n",
    "%timeit a + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 12.2 ms per loop\n",
      "100 loops, best of 3: 10.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "## This demo requires Anaconda distribution to be installed\n",
    "import mkl\n",
    "n = 500\n",
    "a = np.random.randn(n, n)\n",
    "mkl.set_num_threads(1)\n",
    "%timeit a.dot(a)\n",
    "mkl.set_num_threads(2)\n",
    "%timeit a.dot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Typically, two cases are distinguished: \n",
    "1. Shared memory (i.e., multicore on every desktop/smartphone)\n",
    "2. Distributed memory (i.e. each processor has its own memory, can send information through a network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In both cases, the efficiency is governed by a  \n",
    "\n",
    "**memory bandwidth**:  \n",
    "\n",
    "I.e., for BLAS-1,2 routines (like sum of two vectors) reads/writes take all the time.  \n",
    "\n",
    "For BLAS-3 routines, the speedup can be obtained that is more noticable.  \n",
    "\n",
    "For large-scale clusters (>100 000 cores, see the [Top500 list](http://www.top500.org/lists/)) there is still scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communication-avoiding algorithms\n",
    "A new direction in NLA is **communication-avoiding** algorithms (i.e. Hadoop), when you have many computing nodes, but very slow communication with limited communication capabilities.  \n",
    "\n",
    "This requires **absolutely different algorithms**.\n",
    "\n",
    "This can be an interesting **project** (i.e. do NLA in a cloud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of MM part\n",
    "- MM is the core of NLA. You have to think in block terms, if you want high efficiency\n",
    "- This is all about computer memory hierarchy\n",
    "- $\\mathcal{O}(n^{2 + \\epsilon})$ complexity hypothesis is not proven or disproven yet.\n",
    "\n",
    "Now we go to **matrix norms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices and norms\n",
    "How to measure distances between matrices?  \n",
    "\n",
    "A trivial answer is that there is no big differences between matrices and vectors, and here comes the **Frobenius** norm of the matrix:\n",
    "$$\n",
    "  \\Vert A \\Vert_F = \\sum_{i=1}^n \\Big(\\sum_{j=1}^m |a_{ij}|^2\\Big)^{1/2}\n",
    "$$\n",
    "But there is a problem in such definition: this is not a **matrix norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix norms\n",
    "$\\Vert \\cdot \\Vert$ is called a **matrix norm** if it is a vector norm on the linear space of $n \\times m$ matrices, and it also is consistent with the matrix-by-matrix product, i.e.\n",
    "\n",
    "$$\\Vert A B \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert$$\n",
    "\n",
    "The multiplicative property is needed in many places, for example in the estimates for the error of solution of linear systems (we will cover this subject later).   \n",
    "\n",
    "Can you think of some matrix norms, and is Frobenius norm a matrix norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operator norms\n",
    "The most important class of the norms is the class of **operator norms**. Mathematically, they are defined as\n",
    "\n",
    "$$\n",
    "    \\Vert A \\Vert_* = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_*}{\\Vert x \\Vert_*},\n",
    "$$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert_*$ is a **vector norm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix p-norms\n",
    "It is not diffcult to show that operator norm is a matrix norm. Among all operator norms $p$-norms are used, where $p$-norm is used as the vector norm. Among all $p$-norms three norms are the most common ones:  \n",
    "\n",
    "- $p = 2, \\quad$ spectral norm, denoted by $\\Vert A \\Vert_2$.\n",
    "- $p = \\infty, \\quad \\Vert A \\Vert_{\\infty} = \\max_i \\sum_j |A_{ij}|$.\n",
    "- $p = 1, \\quad \\Vert A \\Vert_{1} = \\max_j \\sum_i |A_{ij}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral norm\n",
    "Spectral norm, $\\Vert A \\Vert_2$ is undoubtedly the most used matrix norm. It can not be computed directly from the entries using a simple formula, like the Euclidean norm, however, there are efficient algorithm to compute it.  It is directly related to the **singular value decomposition** (SVD) of the matrix. It holds\n",
    "\n",
    "$$\n",
    "   \\Vert A \\Vert_2 = \\sigma_1(A)\n",
    "$$\n",
    "\n",
    "where $\\sigma_1(A)$ is the largest singular value of the matrix $A$. We will soon learn all about this. Meanwhile, we can already compute the norm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral: 19.7089521303 Frobenius: 100.396005941 1-norm 93.761637223 infinity 96.0482085766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.random.randn(n, n) #Random n x n matrix\n",
    "s1 = np.linalg.norm(a, 2) #Spectral\n",
    "s2 = np.linalg.norm(a, 'fro') #Frobenius\n",
    "s3 = np.linalg.norm(a, 1) #1-norm\n",
    "s4 = np.linalg.norm(a, np.inf) #It was trick to find the infinity\n",
    "print 'Spectral:', s1, 'Frobenius:', s2, '1-norm', s3, 'infinity', s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalar product\n",
    "If norm is a measure of distances, then the scalar product takes angle into account.  \n",
    "\n",
    "The scalar product is defined as\n",
    "$$\n",
    "   (x, y) = \\sum_{i=1}^n \\overline{x}_i y_i,\n",
    "$$\n",
    "where $\\overline{x}$ denotes the *complex conjugate* of $x$. The Euclidean norm is then\n",
    "\n",
    "$$\n",
    "   \\Vert x \\Vert^2 = (x, x),\n",
    "$$\n",
    "\n",
    "or it is said the the norm is **induced** by scalar product.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remark**. For the angle between two vectors is defined as\n",
    "$$\n",
    "   \\cos \\phi = \\frac{(x, y)}{\\Vert x \\Vert \\Vert y \\Vert} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important property of the scalar product is the **Cauchy-Bunyakovski inequality**:\n",
    "$$\n",
    "    (x, y) \\leq \\Vert x \\Vert \\Vert y \\Vert,\n",
    "$$\n",
    "and thus the angle between two vectors is defined properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The scalar product can be written as a matrix-by-matrix product  \n",
    "$$\n",
    "  (x, y) = x^* y,\n",
    "$$\n",
    "where $^*$ is a **conjugate transpose** of the matrix:  \n",
    "$$\n",
    "B = A^*, \\quad B_{ij} = \\overline{A_{ji}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Norm conservation\n",
    "For stability it is really important that the error does not grow. Suppose that you approximately get a vector,  \n",
    "\n",
    "$$\n",
    "  \\Vert x - \\widehat{x} \\Vert \\leq \\varepsilon.\n",
    "$$\n",
    "Let  final result is (some) linear transformation of $x$:  \n",
    "$$\n",
    "   y = Ux, \\quad \\widehat{y} = U \\widehat{x}.\n",
    "$$\n",
    "If we want to estimate a difference between $\\widehat{y}$ and $y$:  \n",
    "\n",
    "$$\n",
    "   \\Vert y - \\widehat{y} \\Vert = \\Vert U ( x - \\widehat{x}) \\Vert \\leq \\Vert U \\Vert \\varepsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices, preserving the norm\n",
    "The question is for which kind of matrices the norm of the vector **will not change**.  \n",
    "\n",
    "For the euclidean norm this produces a very important class of matrices: **unitary** (or orthogonal in the real case) matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary (orthogonal) matrices\n",
    "Let $U$ be an $n \\times r$ matrix, and $\\Vert U z \\Vert = \\Vert z \\Vert$ for all $z$. This can happen if and only if  \n",
    "\n",
    "$$\n",
    "   U^* U = I,\n",
    "$$\n",
    "\n",
    "where $I$ is an **identity matrix**\n",
    "\n",
    "Indeed, $$\\Vert Uz \\Vert^2 = (Uz, Uz) = (Uz)^* Uz = z^* (U^* U) z = z^* z,$$ \n",
    "\n",
    "which can also hold if $U^* U = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary matrices\n",
    "In the real case, when $U^* = U^{\\top}$, the matrix is called orthogonal. \n",
    "\n",
    "Are there many unitary matrices? First of all, **a product of two unitary matrices is a unitary matrix:**  \n",
    "\n",
    "$$(UV)^* UV = V^* (U^* U) V = V^* V = I,$$\n",
    "\n",
    "thus if we give some non-trivial examples of unitary matrices, we will be able to get any unitary transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of unitary matrices\n",
    "There are two important classes of unitary matrices, using those we can make any unitary matrix\n",
    "1. Householder matrices\n",
    "2. Givens (Jacobi) matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Householder matrices\n",
    "Householder matrix is the matrix of the form  \n",
    "$$H = I - 2 vv^*,$$\n",
    "where $u$ is an $n \\times 1$ matrix and $v^* v = I$. Can you show that $H$ is unitary?  It is also a reflection. <img src=\"householder.jpeg\">  \n",
    "A simple proof: $H^* H = (I - 2 vv^*)(I - 2 v v^*) = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Givens(Jacobi) matrix\n",
    "A Givens matrix is a matrix  \n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "          \\cos \\alpha & \\sin \\alpha \\\\\n",
    "          -\\sin \\alpha & \\cos \\alpha\n",
    "        \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which is a rotation. For a general case, we select two $(i, j)$ planes and rotate only in those:  \n",
    "\n",
    "$$\n",
    "    x'_i = \\cos \\alpha x_i + \\sin \\alpha x_j, \\quad x'_j = -\\sin \\alpha x_i + \\cos\\alpha x_j,\n",
    "$$\n",
    "\n",
    "with all other $x_i$ remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary on unitary matrices\n",
    "- Unitary matrices preserve the norm\n",
    "- There are two \"basic\" classes of unitary matrices, Householder and Givens.\n",
    "- Every unitary matrix can be represented as a product of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Matrix multiplication, idea of blocking, memory hiearchy\n",
    "- Scalar product, unitary matrices, basic classes of unitary matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week\n",
    "- TA week\n",
    "- You got PSet 1\n",
    "- Think of course projects (i.e. oil&gas, power networks, social networks)\n",
    "- There is a course on Mathematics of the Internet going now at Skoltech, you are welcome to visit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
