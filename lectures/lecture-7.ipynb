{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7: Linear systems, inverses, condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Matrices, vectors, matrix/vector norms, scalar products & unitary matrices  \n",
    "**Week 2:** TAs-week (Strassen, FFT, a bit of SVD)  \n",
    "**Week 3:** Matrix ranks, singular value decomposition, linear systems  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "Today we will talk about:\n",
    "- Rank of the matrix, idea of linear dependence\n",
    "- Singular value decomposition (SVD) and low-rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "Today we talk about: \n",
    "- Linear systems\n",
    "- Inverse matrix\n",
    "- Condition number\n",
    "- Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear equations and matrices\n",
    "A linear system of equations can be written in the form\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    &2 y + 3 x = 5 \\longrightarrow \\quad &2x + 3 y + 0 z = 5\\\\\n",
    "    &2 x + 3z = 5 \\longrightarrow\\quad &2 x + 0 y + 3 z = 5\\\\\n",
    "    &x + y = 2 \\longrightarrow\\quad  & 1 x + 1 y + 0 z = 0\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 3 & 0 \\\\\n",
    "2 & 0 & 3 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "5 \\\\\n",
    "5 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The \"matrix form\" is\n",
    "$$\n",
    "A x = f, \n",
    "$$\n",
    "\n",
    "where $A$ is a $3 \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems in applications\n",
    "Linear systems are everywhere - in modelling, even if you have nonlinear systems, by **linearization** they are reduced to a sequence of linear systems. They appear in different applications:\n",
    "- Circuit modelling (Kirchoffs law)\n",
    "- Photonics (Maxwell equation, electrodynamics)\n",
    "- Computational fluid dynamics (Navier-Stokes equation)\n",
    "- +$\\infty$ more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems are big\n",
    "We take a continious problem, discretize on a mesh with $N$ elements and get a linear system.  \n",
    "Example of a mesh around A319 aircraft\n",
    "(taken from [GMSH website](http://geuz.org/gmsh/)).  \n",
    "<img src=\"a319_4.png\" width=50%>\n",
    "The main difficulty is that these systems are big: millions or billions of unknowns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems are structured\n",
    "Storing $N^2$ elements of a matrix is prohibitive even for $N = 100000$.  \n",
    "\n",
    "How to work with such matrices?  \n",
    "\n",
    "Fortunately, those matrices are **structured** and require $\\mathcal{O}(N)$ parameters to be stored.  \n",
    "\n",
    "The most widespread structure are **sparse matrices**: in the matrix there are $\\mathcal{O}(N)$ non-zeros!  \n",
    "\n",
    "Example (one of the famous matrices around for $n = 5$):\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "  2 & -1 & 0 & 0 & 0 \\\\\n",
    "  -1 & 2 & -1 & 0 & 0 \\\\\n",
    "  0 & -1 & 2 & -1 & 0 \\\\\n",
    "  0 & 0 &-1& 2 & -1  \\\\\n",
    "  0 & 0 & 0 & -1 & 2 \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "At least you can store such matrices, and multiply by vector fast; but how to solve linear systems?! (and that is typically the final goal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to solve linear systems\n",
    "**Important**: forget about determinants and the **Cramer rule** (it is  good for $2 \\times 2$ matrices still)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to solve linear systems\n",
    "\n",
    "The main tool is variable elimination. \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    &2 y + 3 x = 5 \\longrightarrow \\quad &y = 5/2 -  3/2 x \\\\\n",
    "    &2 x + 3z = 5 \\longrightarrow\\quad &z = 5/3 - 2/3 x\\\\\n",
    "    &x + y = 2 \\longrightarrow\\quad  & 5/2 + 5/3 - (3/2 + 2/3) x = 2,\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "and that is how you find $x$ (and all previous ones).  \n",
    "\n",
    "This process is called **Gaussian elimination** and is one of the most widely used algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian elimination\n",
    "Gaussian elimination consists of two steps:\n",
    "1. Forward step\n",
    "2. Backward step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward step\n",
    "In the forward step, we eliminate $x_1$:\n",
    "$$\n",
    "   x_1 = f_1 - (a_{12} x_2 + \\ldots + a_{1n} x_n)/a_{11},\n",
    "$$\n",
    "and then substitute this into the equations $2, \\ldots, n$. \n",
    "Then we eliminate $x_2$ and so on from the second equation. The important thing is that the **pivots** (that we divide over) are not equal to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward step\n",
    "In the backward step, we solve equation for $x_n$, put it into the equation for $x_{n-1}$ and so on, until we \n",
    "compute all $x_i, i=1,\\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexity of the Gaussian elimination\n",
    "Each elimination step requires $\\mathcal{O}(n^2)$ operations. Thus, the cost of the naive algorithm is $\\mathcal{O}(n^3)$.  \n",
    "**Think a little bit**: Can Strassen help here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian elimination and LU decomposition\n",
    "Gaussian elimination is the computation of one of the most important matrix decompositions: **LU-decomposition**\n",
    "\n",
    "**Definition** LU-decomposition of the matrix is the representation\n",
    "\n",
    "$$A =  LU,$$\n",
    "\n",
    "where $L$ is **lower triangular** and $U$ is **upper triangular** matrices (i.e. elements strictly above the diagonal are zero, elements strictly below the diagonal are zero)\n",
    "\n",
    "The forward step is the computation of $z = L^{-1} b$, and the backward step is the computation of $U^{-1} z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing LU-decomposition\n",
    "In many cases, computing LU-decomposition once is a good idea!\n",
    "\n",
    "Once the decomposition is found (it is $\\mathcal{O}(n^3)$), then solving linear systems with $L$ and $U$ costs only $\\mathcal{O}(n^2)$ operations.\n",
    "\n",
    "**Check:** Solving linear systems with triangular matrices is easy (why?). How we compute the $L$ and $U$ factors? \n",
    "\n",
    "Not let us see how it works in numpy (for the Hilbert matrix, of course).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "n = 5\n",
    "a = [[1.0/(i + j + 1) for i in xrange(n)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.ones(n) #Right-hand side\n",
    "x = np.linalg.solve(a, rhs)\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(x) - rhs) / np.linalg.norm(rhs)\n",
    "plt.xkcd()\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you see, the error grows with larger $n$, and we have to find out why.  \n",
    "**Important point** is that it is not a problem of the algorithm: it is a problem of representing  \n",
    "the matrix in the memory. The error occurs in the moment when the matrix elements are evaluated approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems and inverse matrix\n",
    "What was the problem in the previous example? Why the error grows so quickly?  \n",
    "And here is one of the main concepts of numerical linear algebra: the concept of **condition number** of a matrix.  \n",
    "But before that we have to define the **inverse**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse: definition\n",
    "The inverse of a matrix $A$ is defined as a matrix $X$ denoted by $A^{-1}$ such that  \n",
    "$$\n",
    "   AX = XA = I, \n",
    "$$\n",
    "where $I$ is the identity matrix (i.e., $I_{ij} = 0$ if $i \\ne j$ and $1$ otherwise).\n",
    "The computation of the inverse is linked to the solution of linear systems.  Indeed, $i$-th column of the product gives  \n",
    "$$\n",
    "A x_i = e_i,\n",
    "$$\n",
    "where $e_i$ is the $i$-th column of the identity matrix. Thus, we can apply Gaussian elimination to solve this system. Moreover, if there are no divisions by zero in this process (and the pivots do not depend on the right-hand side), the it is possible to solve the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse matrix and linear systems\n",
    "If we have computed $A^{-1}$, the solution of linear system  \n",
    "$$Ax = f$$\n",
    "is just $x = A^{-1} f$.  \n",
    "Indeed,  \n",
    "$$   \n",
    "    A(A^{-1} f) = (AA^{-1})x = I x = f.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neumann series \n",
    "To study, why there can be such big errors in a solution (see the example above on the Hilbert matrix)  we need an important auxilary result:  **Neumann series**:  \n",
    "If for a matrix $\\Vert F \\Vert < 1$ then the matrix $(I - F)$ is invertible and\n",
    "$$(I - F)^{-1} = I + F + F^2 + F^3 + \\ldots = \\sum_{k=0}^{\\infty} F^k.$$\n",
    "Note that it is a matrix version of the geometric progression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof\n",
    "The proof is constructive. First of all, the show prove that the series $\\sum_{k=0}^{\\infty} F^k$ converges.  \n",
    "Like in the scalar case, we have  \n",
    "$$\n",
    "   (I - F) \\sum_{k=0}^N F^k = (I - F^{N+1}) \\rightarrow I\n",
    "$$\n",
    "We can also estimate the **norm of the inverse**:\n",
    "$$\n",
    "  \\Vert \\sum_{k=0}^N F^k \\Vert \\leq \\sum_{k=0}^N \\Vert F \\Vert^k \\Vert I \\Vert = \\frac{\\Vert I \\Vert}{I - \\Vert F \\Vert} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Small perturbation of the inverse\n",
    "Using this result, we can estimate, how the perturbation of the matrix influences the inverse matrix. We assume\n",
    "that the perturbation $E$ is small in the sense that $\\Vert A^{-1} E \\Vert < 1$. Then\n",
    "$$(A + E)^{-1} = \\sum_{k=0}^{\\infty} (-A^{-1} E)^k A^{-1}$$\n",
    "and moreover, \n",
    "$$\n",
    "  \\frac{\\Vert (A + E)^{-1} - A^{-1} \\Vert}{\\Vert A^{-1} \\Vert} \\leq \\frac{\\Vert A^{-1} \\Vert \\Vert E \\Vert}{1 - \\Vert A^{-1} E \\Vert}.\n",
    "$$\n",
    "As you see, the norm of the inverse enters the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Condition number of a linear system\n",
    "Now consider the **perturbed** linear system:\n",
    "$$\n",
    "   (A + \\Delta A) \\widehat{x} = f + \\Delta f.\n",
    "$$\n",
    "Then the algebra begins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimates\n",
    "$\\widehat{x} - x = (A + \\Delta A)^{-1} (f + \\delta f) - A^{-1} f = ((A + \\Delta A)^{-1} - A^{-1})f + (A + \\Delta A)^{-1} \\Delta f = $  \n",
    "$ = \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k\\Big] A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (A^{-1} \\Delta A)^k \\Big] A^{-1} \\Delta f,$  \n",
    "therefore\n",
    "$\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\Vert A^{-1}\\Delta A\\Vert}\\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big)$.\n",
    "\n",
    "The crucial role is played by the **condition number** $\\mathrm{cond}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert$.  \n",
    "The larger the condition number, the less number of digits we can recover.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The spectral norm of the matrix is equal to **largest singular value**, and the singular values of the inverse matrix are equal to the inverses of the singular values.  Thus,  the condition number is equal to the ratio of the largest singular value and the smallest singular value.\n",
    "$$\n",
    "    \\mathrm{cond(A)} = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hilbert matrix (again)\n",
    "We can also try to test how tight is the estimate, both with ones in the right-hand side, and with a random vector in the right-hand side. The results are strickingly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "n = 20\n",
    "a = [[1.0/(i + j + 1) for i in xrange(n)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.ones(n) #Right-hand side\n",
    "f = np.linalg.solve(a, rhs)\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n",
    "cn = np.linalg.cond(a)\n",
    "print 'Error:', er, 'Condition number:', cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And with random right-hand side..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 20\n",
    "a = [[1.0/(i + j + 1) for i in xrange(n)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.random.randn(n) #Right-hand side\n",
    "f = np.linalg.solve(a, rhs)\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n",
    "cn = np.linalg.cond(a)\n",
    "print 'Error:', er, 'Condition number:', cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** Can you think about an explanation?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overdetermined linear systems\n",
    "Important class of problems are **overdetermined linear systems**, when the number of equations is greater, than the number of unknowns. The simplest example that you all know, is **linear fitting**, fitting a set of 2D points by a line.\n",
    "\n",
    "Then, a typical way is to minimize the **residual**\n",
    "\n",
    "$$\\Vert A x - b \\Vert \\rightarrow \\min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overdetermined system and Gram matrix\n",
    "\n",
    "A simple calculation shows: \n",
    "\n",
    "$(A x - b, A \\delta X) = 0, \\quad A^* A x = A^* b$\n",
    "\n",
    "The matrix $A^* A$ is called **Gram matrix** and the system is called **normal equations**. \n",
    "\n",
    "This is not a good way to do it, since the condition number of $A^* A$ is a square of condition number of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other ways to solve linear least squares\n",
    "A canonical way is to use QR-factorization:  \n",
    "\n",
    "any matrix can be factored into a product \n",
    "\n",
    "$$A = Q R, $$\n",
    "where $Q$ is unitary, and $R$ is upper triangular (details in the next lectures).\n",
    "\n",
    "Then, finding optimal $x$ is equivalent to solving $Rx = Q^* b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding into a bigger system\n",
    "\n",
    "Instead of solving $A^* A x = A^* b$, \n",
    "\n",
    "we introduce a new variable $r = Ax - b$ and then have\n",
    "\n",
    "$$A^* r = 0, \\quad r = Ax - b,$$\n",
    "\n",
    "or in the block form\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  0 & A^* \\\\\n",
    "  A & -I \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "r\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "b\n",
    "\\end{bmatrix},\n",
    "$$  \n",
    "\n",
    "the total size of the system is $(n + m)$ square, and the condition number is the same as for $A$ (by the way, how we define the condition number of a rectangular matrix?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudoinverse\n",
    "The solution can be written as\n",
    "$$x = A^{\\dagger} b.$$\n",
    "\n",
    "The matrix $A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(A^* A + \\alpha I)^{-1} A^*$ is called **Moore-Penrose** pseudoinverse of the matrix $A$\n",
    "\n",
    "It satisfies $A^{\\dagger} A = I$, but $A A^{\\dagger} \\ne I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVD, Pseudoinverse, condition number\n",
    "Let $A = U \\Sigma V^*$ be the SVD of $A$. Then,\n",
    "\n",
    "$$A^{\\dagger} = (A^* A)^{-1}A^* = (V \\Sigma^2 V^*)^{-1} V \\Sigma U^* = V \\Sigma^{\\dagger} U^*,$$\n",
    "\n",
    "where $\\Sigma^{\\dagger}$ consists of inverses of non-zero singular values of $A$.\n",
    "\n",
    "Thus, the condition number is just the ratio of largest and smallest non-zero singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lacking for structure\n",
    "A typical 3D-problem requires a $100 \\times 100 \\times 100$ discretization  \n",
    "\n",
    "This gives a linear system with $10^6$ unknowns, takes $8$ megabytes of memory.\n",
    "\n",
    "This matrix has $10^6 \\times 10^6 = 10^{12}$ elements, takes $8$ terabytes of memory.\n",
    "\n",
    "Fortunately, the matrices in real-life are not **dense**, but have certain **structure**:\n",
    "\n",
    "- Sparse matrices\n",
    "- Low-rank matrices\n",
    "- Toeplitz matrices (shift-invariant property\n",
    "- Sparse in certain bases\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Linear systems can be solved by Gaussian elimination, complexity in $\\mathcal{O}(n^3)$.\n",
    "- Linear systems can be solved by LU-decomposition, complexity is $\\mathcal{O}(n^3)$ for the decomposition, $\\mathcal{O}(n^2)$ for each solve\n",
    "- Linear least squares can be solved by normal equation (bad)\n",
    "- Linear least squares can be solved by QR-decomposition (good) or by augmentation (not bad)\n",
    "- Without structure, we can solve up to $10^4$ linear systems on a laptop (memory restrictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Eigenvectors, eigenvalues, Schur theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
