{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: Iterative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Matrices, vectors, matrix/vector norms, scalar products & unitary matrices  \n",
    "**Week 2:** TAs-week (Strassen, FFT, a bit of SVD)  \n",
    "**Week 3:** Matrix ranks, singular value decomposition, linear systems, eigenvalues  \n",
    "**Week 4:** Matrix decompositions: QR, LU, SVD + test + structured matrices start  \n",
    "**Week 5:** Iterative methods, preconditioners, matrix functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Test\n",
    "- Short intro into sparse matrices and their storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "\n",
    "Today we will talk about **iterative methods**, where they arise, how we store them, how we operate with them.\n",
    "\n",
    "\n",
    "- Concept of iterative methods\n",
    "- Richardson iteration, Chebyshev acceleration\n",
    "- Jacobi/Gauss-Seidel methods\n",
    "- Idea of Krylov methods\n",
    "- Conjugate gradient methods for symmetric positive definite matrices\n",
    "- Generalized minimal residual methods\n",
    "- The Zoo of iterative methods: BiCGStab, QMR, IDR, ...\n",
    "- The idea of preconditioners\n",
    "- Jacobi/Gauss-Seidel as preconditioner, successive overrelaxation\n",
    "- Incomplete ILU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct methods\n",
    "\n",
    "Iterative methods are the main tools for solving large-scale numerical linear algebra problems. \n",
    "\n",
    "$$Ax = f,$$\n",
    "\n",
    "If the matrix $A$ is **dense**, the complexity is $\\mathcal{O}(N^3)$, and is feasible only up to $N \\sim 10^4 - 10^5$.\n",
    "\n",
    "If the matrix $A$ is **sparse**, the complexity is $\\mathcal{O}(N^{\\alpha})$, where $\\alpha = \\frac{3}{2}$ for 2D problems, and $\\alpha = \\frac{5}{3}$ for 3D problems (non-optimal).\n",
    "\n",
    "**But** the matrix-by-vector product is computed in $\\mathcal{O}(N)$ operations, **can we use it**?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix as a black box\n",
    "\n",
    "We have now an absolutely different view on a matrix: matrix is now a **linear operator**, that acts on a vector,  \n",
    "\n",
    "and this action can be computed in $\\mathcal{O}(N)$ operations.\n",
    "\n",
    "**This is the only information** we know about the matrix: the <font color='red'> matrix-by-vector product </font>\n",
    "\n",
    "Can we solve linear systems?\n",
    "\n",
    "Of course, we can multiply by the colums of the identity matrix, and recover the full matrix, but it is not what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Richardson iteration\n",
    "The simplest idea is the **\"simple iteration method\"** or **Richardson iteration**.  \n",
    "\n",
    "\n",
    "  $$Ax = f,$$\n",
    "  $$\\tau  (Ax - f) = 0,$$\n",
    "   $$x + \\tau (Ax - f) = x,$$\n",
    "   $$x_{k+1} = x_k + \\tau (Ax_k - f),$$\n",
    "   \n",
    "   where $\\tau$ is the **iteration parameter**, which can be always chosen such that the method **converges**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence of the Richardson method\n",
    "Let $x_*$ be the solution; introduce an error $e_k = x_{k} - x_*$, then  \n",
    "\n",
    "$$\n",
    "     e_{k+1} = (I + \\tau A) e_k,\n",
    "$$\n",
    "\n",
    "therefore if $\\Vert I + \\tau A \\Vert < 1$ the iteration converges. \n",
    "\n",
    "For symmetric positive definite case it is always possible to select $\\tau.$\n",
    "\n",
    "What about the non-symmetric case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal parameter choice\n",
    "The optimal choice for $\\tau$ for $A = A^* > 0$ is (prove it!)\n",
    "$$\n",
    "  \\tau = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}.\n",
    "$$\n",
    "\n",
    "where $\\lambda_{\\min}$ is the minimal eigenvalue, and $\\lambda_{\\max}$ is the maximal eigenvalue of the matrix $A$.\n",
    "\n",
    "So, to find optimal parameter, we need to know the **bounds of the spectra** of the matrix $A$,\n",
    "\n",
    "and we can compute it by using **power method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connection to ODEs\n",
    "\n",
    "The Richardson iteration has a deep connection to the Ordinary Differential Equations (ODE).\n",
    "\n",
    "\n",
    "Consider a time-dependent problem\n",
    "\n",
    "$$\\frac{dy}{dt} = A y - f, \\quad y(0) = y_0.$$\n",
    "\n",
    "Then $y(t) \\rightarrow A^{-1} f$ as $t \\rightarrow \\infty$, and the **Euler scheme** reads\n",
    "\n",
    "$$\\frac{(y_{k+1} - y_k)}{\\tau} = A y_k - f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence speed and condition number\n",
    "\n",
    "Even with the optimal parameter choice, the error at the next step satisfies\n",
    "\n",
    "$$e_{k+1} \\leq e_k q, \\rightarrow e_k \\leq c q^k,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "   q = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{c - 1}{c+1},\n",
    "$$\n",
    "\n",
    "$$c = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\mathrm{cond}(A)$$\n",
    "\n",
    "is the condition number of $A$.\n",
    "\n",
    "Let us do some demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg as spla\n",
    "import scipy\n",
    "from scipy.sparse import csc_matrix\n",
    "n = 20\n",
    "ex = np.ones(n);\n",
    "lp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
    "rhs = np.ones(n)\n",
    "ev1, vec = spla.eigs(lp1, k=2, which='LR')\n",
    "ev2, vec = spla.eigs(lp1, k=2, which='SR')\n",
    "lam_max = ev1[0]\n",
    "lam_min = ev2[0]\n",
    "\n",
    "tau_opt = 2.0/(lam_max + lam_min)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.close(fig)\n",
    "\n",
    "niters = 100\n",
    "x = np.zeros(n)\n",
    "res_all = []\n",
    "for i in xrange(niters):\n",
    "    rr = lp1.dot(x) - rhs\n",
    "    x = x - tau_opt * rr\n",
    "    res_all.append(np.linalg.norm(rr))\n",
    "#Convergence of an ordinary Richardson (with optimal parameter)\n",
    "plt.plot(res_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Condition number of and convergence speed\n",
    "Thus, for **ill-conditioned** matrices the error of the simple iteration method decays very slowly.\n",
    "\n",
    "This is another reason why **condition number** is so important:\n",
    "\n",
    "1. It gives the bound on the error in the solution\n",
    "\n",
    "2. It gives an estimate of the number of iterations for the iterative methods.\n",
    "\n",
    "Main questions for the iterative method is how to make the matrix **better conditioned** \n",
    "\n",
    "The answer is <font color='red'> use preconditioners </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Better iterative methods\n",
    "\n",
    "But before preconditioners, we need to use **better iterative methods**. \n",
    "\n",
    "There is a whole **zoo** of iterative methods, but we need to know just few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attempt 1: Different time steps\n",
    "\n",
    "Suppose we **change** $\\tau$ every step, i.e. \n",
    "$$\n",
    "   x_{k+1} = x_k + \\tau_k (A x_k - f).\n",
    "$$\n",
    "\n",
    "Then, $$e_{k+1} = (I - \\tau_k A) e_k = (I - \\tau_k A) (I - \\tau_{k-1} A)  e_{k-1} = \\ldots = p(A) e_0, $$\n",
    "\n",
    "where $p(A)$ is a **matrix polynomial** (simplest matrix function)  \n",
    "\n",
    "$$\n",
    "   p(A) = (I - \\tau_k A) \\ldots (I - \\tau_0 A),\n",
    "$$\n",
    "\n",
    "and $p(0) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal choice of the time steps\n",
    "The error is written as \n",
    "\n",
    "$$e_{k+1} = p(A) e_0, $$\n",
    "\n",
    "where $p(0) = 1$ and $p(A)$ is a **matrix polynomial**. \n",
    "\n",
    "To get better **error reduction**, we need to minimize\n",
    "\n",
    "$$\\Vert p(A) \\Vert$$ over all possible polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Polynomials least deviating from zeros\n",
    "\n",
    "Important special case: $A = A^* > 0$.\n",
    "\n",
    "Then $A = U \\Lambda U^*$, \n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "\n",
    "$$\\Vert p(A) \\Vert = \\Vert U p(\\Lambda) U^* \\Vert = \\Vert p(\\Lambda) \\Vert = \\max_k |p(\\lambda_k)| \\leq \n",
    "\\max_{a \\leq \\lambda \\leq b} p(\\lambda).$$\n",
    "\n",
    "Thus, we need to find a polynomial such that $p(0) = 1$, that has the least possible deviation from $0$ on a given interval $[a, b]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Polynomials least deviating from zeros (2)\n",
    "\n",
    "We can do the affine transformation of the interval $[a, b]$ to the interval $[-1, 1]$.\n",
    "\n",
    "The problem is then reduced to the problem of finding the **polynomial least deviating from zero** on an interval $[-1, 1]$ \n",
    "\n",
    "with some normalization constraint $p(c) = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact solution: Chebyshev polynomials\n",
    "\n",
    "The exact solution to this problem is given by the famous **Chebyshev polynomials** of the form\n",
    "\n",
    "$$T_n(x) =  \\cos (n \\arccos x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do you need to know about Chebyshev polynomials\n",
    "\n",
    "1. This is a polynomial! (we can express $T_n$ from $T_{n-1}$ and $T_{n-2}$).\n",
    "\n",
    "2. $|T_n(x)| \\leq 1$ on $x \\in [-1, 1]$.\n",
    "\n",
    "3. It has $(n+1)$ **alternation points**, were the the maximal absolute value is achieved (this is the sufficient and necessary condition for the **optimality**\n",
    "\n",
    "4. The **roots** are just  \n",
    "$n \\arccos x_k = \\frac{\\pi}{2} + \\pi k, \\rightarrow x_k = \\cos \\frac{\\pi(k + 0.5)}{n}$\n",
    "\n",
    "We can plot them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()\n",
    "%matplotlib inline\n",
    "x = np.linspace(-1, 1, 128)\n",
    "p = np.polynomial.Chebyshev((0, 0, 0, 0, 0, 0, 0, 1), (-1, 1)) #These are Chebyshev series, a proto of \"chebfun system\" in MATLAB\n",
    "plt.plot(x, p(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence of the Chebyshev-accelerated Richardson iteration\n",
    "\n",
    "Given the roots of the polynomials $x_k$, the best parameters are determined as \n",
    "\n",
    "$$\\tau_k = \\frac{1}{x_k}.$$\n",
    "\n",
    "The convergence (we only give the result without the proof) is now given by\n",
    "\n",
    "$$\n",
    "   e_{k+1} \\leq c q^k, \\quad q = \\frac{\\sqrt{c}-1}{\\sqrt{c}+1},\n",
    "$$\n",
    "\n",
    "where $c = \\mathrm{cond}(A)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Chebyshev\n",
    "\n",
    "We have made an important assumption about the spectra: it is contained within an interval over the real line (and we need to know the bounds)\n",
    "\n",
    "If the spectra is contained within **two intervals**, and we know the bounds, we can also put the optimization problem \n",
    "\n",
    "for the **optimal polynomial**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectra of the matrix contain on multiple segments\n",
    "\n",
    "For the case of **two segments** the best polynomial is given by **Zolotarev polynomials** (expressed in terms of elliptic functions)\n",
    "\n",
    "For the case of **more than two segments** the best polynomial can be expressed in terms of **hyperelliptic functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img width=100% src='WAT.jpeg'> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we make it better\n",
    "\n",
    "The implementation of the Chebyshev acceleration requires the knowledge of the spectra.\n",
    "\n",
    "It only stores the **previous vector** $x_k$ and computes the new correction vector\n",
    "\n",
    "$$p_k = A x_k - f$$.\n",
    "\n",
    "It belongs to the class of **two-term** iterative methods.\n",
    "\n",
    "It appears that if we **abandon** this and **store more vectors**, then we can go without the spectra estimation (and better convergence in practice)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Crucial point\n",
    "\n",
    "The Chebyshev method produces the approximation of the form\n",
    "\n",
    "$$x_{k+1} = p(A) f,$$\n",
    "\n",
    "i.e. it lies in the the **Krylov subspace** of the matrix  which is defined as\n",
    "\n",
    "$$\n",
    "   K(A, f) = \\mathrm{Span}(f, Af, A^2 f, \\ldots, )\n",
    "$$\n",
    "\n",
    "The most natural approach then is to find the vector in this **linear subspace** that minimizes \n",
    "\n",
    "certain **norm of the error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What to minimize in the Krylov subspace\n",
    "\n",
    "The ideal way is to minimize\n",
    "\n",
    "$$\\Vert x_k - x_* \\Vert,$$\n",
    "\n",
    "over the Krylov subspace $K_n(A, f)$\n",
    "\n",
    "where $x_*$ is the **true solution**, but we do not know it!\n",
    "\n",
    "Then we can minimize the **residual**: \n",
    "\n",
    "$$\\Vert f - Ax \\Vert,$$\n",
    "\n",
    "For the SPD case we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symmetric positive definite case\n",
    "\n",
    "Consider again $A = A^* > 0$ \n",
    "\n",
    "Then we can introduce **A-norm** as \n",
    "\n",
    "$$\\Vert x \\Vert_A =  \\sqrt{(Ax, x)}.$$\n",
    "\n",
    "Let us verify that it is the norm (on the whiteboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conjugate gradient\n",
    "\n",
    "The most popular iterative method today (for positive definite matrices) is the **conjugate gradient method**:\n",
    "\n",
    "It minimizes the **A-norm** of the error over the Krylov subspace\n",
    "\n",
    "$$x_{k+1} = \\arg_{x \\in K_n(A, f)} \\min \\Vert x - x_* \\Vert_A$$\n",
    "\n",
    "And we can compute the minimization without knowing the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why we can compute A-norm of the error\n",
    "\n",
    "Given $x$ we can compute the A-norm of the error:\n",
    "\n",
    "$$\\Vert x - x_* \\Vert_A^2 =  (A (x - x_*), (x - x_*))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to get the CG formulas\n",
    "\n",
    "Consider the one step:\n",
    "\n",
    "$x_i = x_0 + y_i, \\quad y_i = \\arg \\min \\Vert x_0 + y - z \\Vert_A, \\quad y \\in K_n$.  \n",
    "\n",
    "Using Pythagoreus theorem,\n",
    "\n",
    "$(x_n, y) = (z, y)_A$ for all $y \\in K_n$, $\\rightarrow$ $r_n = Ax_n - f \\perp K_n$\n",
    "\n",
    "After some machinery..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CG-method: formulas\n",
    "\n",
    "We get the following **three-term** recurrence relations for the CG method:\n",
    "$$\n",
    "   \\alpha_n = (r_{n-1}, r_{n-1})/(Ap_n, p_n),\n",
    "$$\n",
    "$$\n",
    "   x_n = x_{n-1} + \\alpha_n p_n, \n",
    "$$\n",
    "$$\n",
    "   r_n = r_{n-1} - \\alpha_n A p_n,\n",
    "$$\n",
    "$$\n",
    "   \\beta_n = (r_n, r_n)/(r_{n-1},r_{n-1}),\n",
    "$$\n",
    "$$\n",
    "    p_{n+1} = r_n + \\beta_n p_n\n",
    "$$\n",
    "\n",
    "\n",
    "The vectors $p_i$ constitute an A-orthogonal basis in $K_n$.\n",
    "\n",
    "We store $x$, $p$, $r$ (three vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CG-method: history\n",
    "CG-method has a long history:\n",
    "\n",
    "1. In was proposed in by [Hestenes and Stiefel in 1952](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf).\n",
    "2. In exact arithmetics it should give **exact solution** at $n$ iterations\n",
    "3. In floating-point arithmetics it did not - people thought it is **unstable** compared to Gaussian elimination\n",
    "4. Only decades later it was realized that it is a wonderful **iterative method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CG-method: properties\n",
    "\n",
    "1. Convergence estimate (same convergence speed as Chebyshev)\n",
    "  $$\n",
    "     \\Vert e_k \\Vert \\leq 2 \\Big( \\frac{\\sqrt{c} - 1}{\\sqrt{c} + 1} \\Big)^k \\Vert e_0 \\Vert.\n",
    "  $$\n",
    "  \n",
    "2. CG mystery (why it is much better than Chebyshev) is **clustering of eigenvalues:**\n",
    "\n",
    "if there are $k$ \"outliers\", then in $k$ iterations they are \"removed\" (we can illustrate it on the board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-symmetric matrices\n",
    "\n",
    "CG only works for SPD matrices, and (sometimes) for symmetric matrices (although we can divide by zero).\n",
    "\n",
    "It completely **does not work** for the non-symmetric matrices.\n",
    "\n",
    "For non-symmetric matrices the two main methods are\n",
    "\n",
    "1. GMRES (Generalized minimal residual method)\n",
    "2. BiCGStab (BiConjugate Gradient Stabilized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GMRES\n",
    "\n",
    "GMRES idea is  very simple: **we construct orthogonal basis in the Krylov subspace.** \n",
    "\n",
    "Given the basis $q_1, \\ldots, q_n$ we compute the residual $r_ n = Ax_n - f$ and orthogonalize it to the basis.\n",
    "\n",
    "A Python realization is available as ```scipy.sparse.linalg.gmres``` (although quite buggy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantage of GMRES\n",
    "The main disadvantage of GMRES: we have to store all the vectors, so the memory costs grows with each step.  \n",
    "\n",
    "We can do **restarts** (i.e. get a new residual and a new Krylov subspace): we find some approximate solution $x$ \n",
    "\n",
    "and now solve the linear system for the correction:\n",
    "\n",
    "$$A(x + e) = f, \\quad Ae = f - Ax,$$\n",
    "\n",
    "and generate the new **Krylov subspace**.\n",
    "\n",
    "\n",
    "**BiCGStab method** avoids that using \"short recurrences\" like in the CG method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of BiCGStab\n",
    "\n",
    "Idea of BiCG method is to use the normal equations:\n",
    "\n",
    "$$A^* A x = A^* f,$$\n",
    "\n",
    "and apply the CG method to it.\n",
    "\n",
    "The condition number has squared, thus we need **stabilization**.\n",
    "\n",
    "The stabilization idea proposed by Van der Vorst et al. gives the most practically used iterative method for non-symmetric systems.\n",
    "\n",
    "Let us do some demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "n = 50\n",
    "ex = np.ones(n);\n",
    "lp1 = -sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
    "rhs = np.ones(n)\n",
    "ee = sp.sparse.eye(n)\n",
    "\n",
    "#lp2 = sp.kron(lp1, ee) + sp.kron(ee, lp1)\n",
    "#rhs = np.ones(n * n)\n",
    "res_all = []\n",
    "res_all_bicg = []\n",
    "def my_print(r):\n",
    "    res_all.append(r)\n",
    "\n",
    "def my_print2(x): #For BiCGStab they have another callback, please rewrite\n",
    "    res_all_bicg.append(np.linalg.norm(lp1.dot(x) - rhs))\n",
    "    \n",
    "sol = scipy.sparse.linalg.gmres(lp1, rhs, callback=my_print)\n",
    "plt.semilogy(res_all, marker='x',label='GMRES')\n",
    "sol2 = scipy.sparse.linalg.bicgstab(lp1, rhs, callback=my_print2)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Residual')\n",
    "plt.semilogy(res_all_bicg, label='BiCGStab', marker='o')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Battling the condition number\n",
    "\n",
    "The condition number problem is **un-avoidable** if only the matrix-by-vector product is used.\n",
    "\n",
    "Thus we need an **army of preconditioners** to solve it.\n",
    "\n",
    "There are several **general purpose** preconditioners that we can use (short list today, more details tomorrow),  \n",
    "\n",
    "but often for a particular problem a special design is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preconditioner: general concept\n",
    "\n",
    "The general concept of the preconditioner is simple:\n",
    "\n",
    "Given a linear system \n",
    "\n",
    "$$A x = f,$$\n",
    "\n",
    "we want to find the matrix $P$ such that \n",
    "\n",
    "1. We can easily solve $Py = g$ for any $f$\n",
    "2. Condition number of $AP^{-1}$ is better\n",
    "\n",
    "Then we solve for (right preconditioner)\n",
    "\n",
    "$$ AP^{-1} y = f.$$ \n",
    "\n",
    "or  (left preconditioner)\n",
    "\n",
    "$$ P^{-1} A y = f$$ \n",
    "\n",
    "The best choice of course is $$P = A$$, but this does not make life easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other iterative methods as preconditioners\n",
    "There are other iterative methods that we have not mentioned. \n",
    "\n",
    "1. Jacobi method\n",
    "2. Gauss-Seidel method\n",
    "3. SSOR (Successive over-relaxation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jacobi method\n",
    "\n",
    "Jacobi method is when you express the diagonal element:\n",
    "\n",
    "$$a_{ii} x_i = \\sum_{i \\ne j} a_{ij} x_j$$\n",
    "\n",
    "and use this to iteratively update $x_i$.\n",
    "\n",
    "From the matrix viewpoint we use $$D = \\mathrm{diag}(A)$$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gauss-Seidel (as preconditioner)\n",
    "Another well-known method is **Gauss-Seidel method**. Given $A = A^{*} > 0$ we have  \n",
    "\n",
    "$$A = L + D + L^{*},$$\n",
    "\n",
    "where $D$ is the diagonal of $A$, $L$ is lower-triangular part, and Gauss-Seidel is $(L + D)^{-1}$.  \n",
    "\n",
    "**Good news: ** $\\rho(I + (L+D)^{-1} A) < 1, $\n",
    "\n",
    "where $\\rho$ is the spectral radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Successive overrelaxation (as preconditioner)\n",
    "\n",
    "We can even introduce a parameter into the preconditioner, giving a **SSOR** method:\n",
    "\n",
    "$$(D + w L) x = w b - (w U + (w-1) D) x,$$\n",
    "\n",
    "$$P = (D+wL)^{-1} (w U + (w-1) D).$$\n",
    "\n",
    "Optimal selection of $w$ is **not trivial**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Others (but very important, see it tomorrow)\n",
    "\n",
    "- Incomplete LU for sparse matrices (tomorrow)\n",
    "- Algebraic multigrid\n",
    "- Sparse approximate inverse \n",
    "- Domain decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Iterative methods are the core\n",
    "- Krylov subspaces are the core\n",
    "- Condition number is the problem\n",
    "- Conjugate gradient is the best for SPD\n",
    "- GMRES/BiCGStab are the best for non-symmetric\n",
    "- Preconditioners are crucial (and often do not work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- More on preconditioners for sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
