{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8: Eigenvalues (and eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "Week 1: Matrices, vectors, matrix/vector norms, scalar products & unitary matrices\n",
    "Week 2: TAs-week (Strassen, FFT, a bit of SVD)\n",
    "Week 3: Matrix ranks, singular value decomposition, linear systems, eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Linear systems\n",
    "- Gaussian elimination\n",
    "- Sparse matrices\n",
    "- and the condition number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "Today we will talk about:\n",
    "- Eigenvectors and their applications (PageRank)\n",
    "- Gershgorin circles\n",
    "- Computing eigenvectors using power method\n",
    "- Schur theorem\n",
    "- Normal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an eigenvector\n",
    "An vector $x \\ne 0$ is called an **eigenvector** of a square matrix $A$ if there exists a number $\\lambda$ such that  \n",
    "$$\n",
    "   Ax = \\lambda x.\n",
    "$$\n",
    "The number $\\lambda$ is called an **eigenvalue**.  \n",
    "The names **eigenpair** and **eigenproblem** are also used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why eigenvectors and eigenvalues are important\n",
    "Eigenvectors are both important auxiliary tools and also play important role in applications.  \n",
    "To start with all our microworld is governed by the **Schrodinger equation** which is an eigenvalue problem.  \n",
    "$$\n",
    "    H \\psi = E \\psi,\n",
    "$$\n",
    "where $E$ is the ground state energy, $\\psi$ is called wavefunction and $H$ is the Hamiltonian.  \n",
    "More than 50% of the computer power is spent on solving this type of problems for computational material / drug design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues are vibrational frequencies\n",
    "\n",
    "A typical computation of eigenvectors / eigenvectors is for studying \n",
    "\n",
    "- Vibrational computations of mechanical structures\n",
    "- Model order reduction of complex systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo \n",
    "YouTubeVideo(\"xKGA3RNzvKg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google PageRank\n",
    "One of the most famous eigenvectors computation is the **Google PageRank**. \n",
    "It is not actively used by Google nowdays, but it was of the main features in its early stages. The question is how do we rank webpages, which one is important, and which one is not. \n",
    "All we know about the web is which page referrs to which. PageRank is defined by a recursive definition. Denote by $p_i$ the **importance** of the $i$-th page. Then we define this importance as an average value of all importances of all pages that refer to the current page. It gives us a linear system  \n",
    "$$\n",
    "    p_i = \\sum_{j \\in N(i)} \\frac{p_j}{L(j)},\n",
    "$$\n",
    "where $L(j)$ is the number of outgoing links on the $j$-th page, $N(i)$ are all the neighbours. It can be rewritten as  \n",
    "$$\n",
    "   p = G p,\n",
    "$$\n",
    "or as an eigenvalue problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "   Gp = 1 p,\n",
    "$$\n",
    "i.e. the eigenvalue $1$ is already known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Demo\n",
    "We can compute it using some Python packages. \n",
    "We will use ```networkx``` package for working with graphs that can be installed using  \n",
    "```conda install networkx```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use a simple example of Zachary karate club network. This data was collected in 1977, and is a classical social network dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xkcd()\n",
    "import networkx as nx\n",
    "kn = nx.read_gml('karate.gml')\n",
    "nx.draw_networkx(kn) #Draw the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can actually compute the PageRank using the NetworkX built-in function. We also plot \n",
    "the size of the nodes larger if its PageRank is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pr = nx.algorithms.link_analysis.pagerank(kn)\n",
    "pr_vector = [pr[i+1] for i in xrange(len(pr))]\n",
    "pr_vector = np.array(pr_vector) * 5000\n",
    "nx.draw_networkx(kn, node_size=pr_vector, labels=None)\n",
    "plt.tight_layout()\n",
    "plt.title('PageRank nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of the eigenvalues via characteristic equations\n",
    "The eigenvalue problem has the form  \n",
    "$$\n",
    "    Ax = \\lambda x, \n",
    "$$\n",
    "or \n",
    "$$\n",
    "   (A - \\lambda I) x = 0,\n",
    "$$\n",
    "therefore matrix $A - \\lambda I$ has non-trivial nullspace and should be singular. That means, that the **determinant**  \n",
    "\n",
    "$$\n",
    "   p(\\lambda) = \\det(A - \\lambda I) = 0.\n",
    "$$\n",
    "\n",
    "The equation is is called **characteristic equations** and is a polynomial of order $n$.    \n",
    "The $n$-degree polynomial has $n$ complex roots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remember the determinant\n",
    "The determinant of a square matrix $A$ is defined as \n",
    "\n",
    "$$\\det A = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}({\\sigma})\\prod^n_{i=1} a_{i, \\sigma_i},$$\n",
    "\n",
    "where $S_n$ is the set of all **permutations** of the numbers $1, \\ldots, n$,\n",
    "\n",
    "and $\\mathrm{sgn}$ is the **signature** of the permutation ( $(-1)^p$ where $p$ is the number of transpositions to be made).\n",
    "\n",
    "(**Checkout** video \"Executing the determinant\" on the Gilbert Strang lecture at MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of the determinant\n",
    "The determinant has many nice properties:\n",
    "\n",
    "1. $\\det(AB) = \\det(A) \\det(B)$\n",
    "2. If we have one row as a sum of two vectors, the determinant is a sum of two determinant\n",
    "3. \"Minor expansion\": we can expand the determinant through a selected row.\n",
    "\n",
    "If you do it via **minor expansion**, we get **exponential** complexity in $\\mathcal{O}(n).\n",
    "\n",
    "Can we do $\\mathcal{O}(n^3)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues and characteristic equation\n",
    "\n",
    "Now we go back to the eigenvalues.\n",
    "\n",
    "The characteristic equation can be used to compute the eigenvalues, which leads to **naive** algorithm:\n",
    "\n",
    "$$p(\\lambda) = \\det(A - \\lambda I)$$\n",
    "\n",
    "1. Compute coefficients of the polynomial\n",
    "2. Compute the roots\n",
    "\n",
    "**Is this a good idea**?   \n",
    "\n",
    "**Give your ideas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can do a short demo of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 5\n",
    "a = [[1.0 / (i - j + 0.5) for i in xrange(n)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "ev = np.linalg.eigvals(a)\n",
    "#print 'Eigenvalues using Lapack function:', ev\n",
    "#There is a special numpy function for chacteristic polynomial\n",
    "cf = np.poly(a)\n",
    "ev_roots = np.roots(cf)\n",
    "#print 'Coefficients of the polynomial:', cf\n",
    "#print 'Polynomial roots:', ev_roots\n",
    "plt.scatter(ev.real, ev.imag, marker='x', label='Lapack')\n",
    "b = a + 1e-1 * np.random.randn(n, n)\n",
    "ev_b = np.linalg.eigvals(b)\n",
    "plt.scatter(ev_b.real, ev_b.imag, marker='o')\n",
    "#plt.scatter(ev_roots.real, ev_roots.imag, marker='o', label='Brute force')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Real part')\n",
    "plt.ylabel('Imaginary part')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Morale**: \n",
    "\n",
    "- Do not do that, unless you have a reason. \n",
    "- Polynomial rootfinding is very **ill-conditioned** (can be much better, but not with monomials!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gershgorin circles\n",
    "There is a very interesting theorem that sometimes helps to localize the eigenvalues. \n",
    "\n",
    "It is called **Gershgorin theorem**.  \n",
    "\n",
    "It states that all the eigenvalues $\\lambda_i, \\quad i = 1, \\ldots, n$ are located inside the union of **Gershgorin circles** $C_i$,  where $C_i$ is a disk on the complex plane with center $a_{ii}$ and radius  \n",
    "\n",
    "$$r_i = \\sum_{j \\ne i} |a_{ij}|.$$\n",
    "\n",
    "Moreover, if the circles do not intersect, that contain only one eigenvalues. The proof is instructive, since it uses the concepts we looked at the previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof\n",
    "First we need to show, that if the matrix $A$ is strictly diagonally dominant, i.e. \n",
    "\n",
    "$$\n",
    "   |a_{ii}| > \\sum_{j \\ne i} |a_{ij}|,\n",
    "$$\n",
    "then such matrix is non-singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We separate the diagonal part and off-diagonal part, and get  \n",
    "\n",
    "$$\n",
    "    A = D + S,\n",
    "$$\n",
    "\n",
    "and $\\Vert D^{-1} S\\Vert_1 < 1$. Therefore, by using the **Neumann series**, the matrix $D + S$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now the proof follows by contradiction: \n",
    "\n",
    "if any of the eigenvalues lies outside all of the circles, the matrix $(A - \\lambda I)$ \n",
    "\n",
    "is strictly diagonally dominant, and thus is invertible. \n",
    "\n",
    "That means, that $(A - \\lambda I) x = 0$ means $x = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A short demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "n = 30\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "a = [[1.0 / (i - j + 0.5) for i in xrange(n)] for j in xrange(n)]\n",
    "a = np.array(a)\n",
    "#a = np.diag(np.arange(n))\n",
    "#a = a + 0.1*np.random.randn(n, n)\n",
    "u = np.random.randn(n, n)\n",
    "a = np.linalg.inv(u).dot(a).dot(u)\n",
    "xg = np.diag(a).real\n",
    "yg = np.diag(a).imag\n",
    "rg = np.zeros(n)\n",
    "ev = np.linalg.eigvals(a)\n",
    "for i in xrange(n):\n",
    "    rg[i] = np.sum(np.abs(a[i, :])) - np.abs(a[i, i])\n",
    "    crc = plt.Circle((xg[i], yg[i]), radius=rg[i], fill=False)\n",
    "    ax.add_patch(crc)\n",
    "plt.scatter(ev.real, ev.imag, label='x', color='r')\n",
    "plt.axis('equal')\n",
    "ax.set_title('Eigenvalues and Gershgorin circles')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: There are more complicated figures, like Cassini ovals, that include the spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power method\n",
    "We are often interested in the computation of the part of the spectrum, like the largest eigenvalues, smallest eigenvalues. Also it is interesting to note that for the Hermitian matrices $(A = A^*)$ the eigenvalues are always real (prove it!).  \n",
    "\n",
    "A power method is the simplest method for the computation of the largest eigenvalue in modulus. It is also our first example of the **iterative method** and **Krylov method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Power method has the form\n",
    "$$\n",
    "    x_{k+1} = A x_k, \\quad x_{k+1} := \\frac{x_{k+1}}{\\Vert x_{k+1} \\Vert_2}.\n",
    "$$\n",
    "The convergence is geometric, but the convergence ratio is $q^k$, where $q = |\\frac{\\lambda_{2}}{\\lambda_{1}}| \\leq 1$ and $k$ is the number of iterations. It means, the convergence can be artitrary small. To prove it, it is sufficient to consider a $2 \\times 2$ diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a more precise look at the power method when $A$ is Hermitian.\n",
    "In two slides you will learn that every Hermitian matrix is diagonalizable. Therefore, there exists orthonormal basis of eigenvectors $v_1,\\dots,v_n$ such that $Av_i = \\lambda_i v_i$. Let us decompose $x_0$ into a sum of $v_i$ with coefficients $c_i$:\n",
    "$$\n",
    "x_0 = c_1 v_1 + \\dots + c_n v_n.\n",
    "$$\n",
    "Since $v_i$ are eigenvectors, we have\n",
    "$$\n",
    "\\begin{split}\n",
    "x_1 &= \\frac{Ax_0}{\\|Ax_0\\|} = \\frac{c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n}{\\|c_1 \\lambda_1 v_1 + \\dots + c_n \\lambda_n v_n \\|}  \\\\\n",
    "&\\vdots\\\\\n",
    "x_k &= \\frac{Ax_{k-1}}{\\|Ax_{k-1}\\|} = \\frac{c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n}{\\|c_1 \\lambda_1^k v_1 + \\dots + c_n \\lambda_n^k v_n \\|}\n",
    "\\end{split}\n",
    "$$\n",
    "Now you see, that \n",
    "$$\n",
    "x_k = \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\frac{ v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k} + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}}{\\|v_1 + \\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k} + \\dots + \\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}\\|},\n",
    "$$\n",
    "which converges to $v_1$ since $\\left| \\frac{c_1}{|c_1|}\\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\right| = 1$ and $\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\to 0$ if $|\\lambda_2|<|\\lambda_1|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Things to remember about the power method\n",
    "- One step requires one matrix-by-vector product. If the matrix allows for an $\\mathcal{O}(n)$ matvec (for example, it is sparse), \n",
    "  then it is possible.\n",
    "- Convergence can be slow\n",
    "- If only a rough estimate is needed, only a few iterations are needed\n",
    "- The solution vector is in the **Krylov subspace** and has the form $\\mu A^k x_0$, where $\\mu$ is the normalization constant. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First matrix decomposition: the Schur form\n",
    "There is one class of matrices when eigenvalues can be found easily: **triangular matrices**\n",
    "\n",
    "$$\n",
    "  A = \\begin{pmatrix}\n",
    "   \\lambda_1 & * & * \\\\\n",
    "   0 & \\lambda_2 & * \\\\\n",
    "   0 & 0 & \\lambda_3 \\\\\n",
    "  \\end{pmatrix}.\n",
    "$$\n",
    "The eigenvalues of $A$ are $\\lambda_1, \\lambda_2, \\lambda_3$. Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because the determinant is  \n",
    "$$\n",
    "   \\det(A - \\lambda I) = (\\lambda - \\lambda_1) (\\lambda - \\lambda_2) (\\lambda - \\lambda_3).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, computing the eigenvalues of triangular matrices is easy. Now, the unitary matrices come to help. Let $U$ be a unitary matrix, i.e. $U^* U = I$.  Then \n",
    "\n",
    "$$\n",
    "   \\det(A - \\lambda I) = \\det(U (U^* A U - \\lambda I) U^*) = \\det(UU^*) \\det(U^* A U - \\lambda I) = \\det(U^* A U - \\lambda I),\n",
    "$$\n",
    "\n",
    "where we have used the famous multiplicativity property of the determinant, $\\det(AB) = \\det(A) \\det(B)$. It means,  \n",
    "that the matrices $U^* A U$ and $A$ have the same characteristic polynomials, and the same eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we manage to make $U^* A U = T$, where $T$ is **upper triangular** then we are done.  Multplying from the left and the right by $U$ and $U^*$ respectively, we get the desired decomposition:  \n",
    "$$\n",
    "    A = U T U^*.\n",
    "$$\n",
    "This is the celebrated **Schur decomposition**. Recall that unitary matrices mean stability, thus the eigenvalues are computed very accuratedly. The Schur decomposition shows why we need matrix decompositions: it represents a matrix into a product of three matrices with structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Schur theorem**  \n",
    "Every $n \\times n$ matrix can be represented in the Schur form.  \n",
    "** Sketch of the proof**.\n",
    "1. Every matrix has at least $1$ non-zero eigenvector (take a root of characteristic polynomial, $(A-\\lambda I)$ is singular, has    non-trivial nullspace). Let $Ax_1 = \\lambda_1 x_1, \\quad \\Vert x_1 \\Vert_2 = 1$\n",
    "2. We find a Householder matrix $U_1$ such that $U_1 x_1 = e_1$. Then, \n",
    "  $$\n",
    "      U^*_1 A U_1 = \\begin{pmatrix}\n",
    "      1 & *  \\\\\n",
    "      0 & A_2 \n",
    "      \\end{pmatrix},\n",
    "  $$\n",
    "  where $A_2$ is an $(n-1) \\times (n-1)$ matrix. This is called **block triangular form**. We can now work with $A_2$ only and so on.  \n",
    "  **Note**: This is not an algorithm, but a proof that Schur form exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of the Schur theorem\n",
    "Important application of the Schur theorem:  **Normal matrices**.  \n",
    "**Definition.** A matrix is called a **normal matrix**, if  \n",
    "$$\n",
    "    AA^* = A^* A.\n",
    "$$\n",
    "Example: Hermitian matrices, unitary matrices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Theorem**: $A$ is a **normal matrix**, iff $A = U \\Lambda U^*$.  \n",
    "\n",
    "**Sketch of the proof:** One way is obvious (if the decomposition holds, the matrix is normal).  \n",
    "The other is more complicated. Consider the Schur form of the matrix $A$. Then $AA^* = A^*A$ means $TT^* = T^* T$.  \n",
    "By looking at the elements we immediately see, \n",
    "that the only uppertriangular matrix $T$ that satisfies $TT^* = T^* T$ is a diagonal matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we compute the Schur decomposition\n",
    "\n",
    "Everything is fine, but how you compute the Schur form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of todays lecture\n",
    "\n",
    "- Eigenvalues, eigenvectors\n",
    "- Gershgorin theorem\n",
    "- Power method\n",
    "- Schur theorem \n",
    "- Normal matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week\n",
    "Next week we will talk about medium-sized problems and the algorithms hidden in **numpy**:\n",
    "\n",
    "SVD, QR, LU, Schur decomposition. How they are actually computed. \n",
    "\n",
    "Then we will go to **sparse and structured** matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
